Detailed Approach for Voice Conversion





Dataset and Metadata Handling





Dataset: The script utilizes the UrduSER dataset, which contains emotional speech recordings in Urdu from both male and female speakers across seven emotions: Angry, Fear, Boredom, Disgust, Happy, Neutral, and Sad.



Metadata Parsing:





The script initially attempts to load metadata from a CSV file named UrSEC_Description.csv. It searches for columns such as file name, emotion, gender, actor ID, and transcription, accommodating headers that might be in Urdu.



If the CSV file is unavailable or malformed, the script resorts to parsing file names directly. It assumes a naming convention of [Actor_ID]_[Gender_Code]_[Emotion_Code]_[Sequence_Number].wav to extract necessary metadata.



Parallel Pair Creation:





For the voice conversion task, the script creates pairs of male and female audio files that share the same emotion and sequence number, suggesting similar content.



When metadata is available, transcriptions are used to verify content similarity between paired files. Without metadata, the script depends entirely on the file naming convention for pairing.



Audio Preprocessing





Audio Loading: Audio files are loaded using the librosa library with a corrected sample rate of 41100 Hz (originally intended as 41000 Hz).



Length Standardization: Each audio file is either padded with silence or truncated to a uniform length, such as 10 seconds, to ensure consistent input sizes for subsequent processing and model training.



Normalization: The audio is normalized to a peak amplitude of 0.95, ensuring uniform volume levels across all files.



Mel Spectrogram Extraction: Mel spectrograms are generated using 80 Mel bands, a hop length of 256 samples, and a window length of 1024 samples. These parameters capture the spectral content critical for voice conversion.



Data Caching





Processed Data Cache: To optimize performance and avoid repetitive preprocessing, the script caches preprocessed Mel spectrograms using the pickle module. This cache is reused in subsequent runs unless explicitly overridden.



Vocoder Data Cache: Pairs of Mel spectrograms and their corresponding audio waveforms are similarly cached to streamline vocoder training.



Data Generators





Voice Conversion Generator: A custom Sequence class is implemented to generate batches of paired male and female Mel spectrograms, facilitating efficient training of the voice conversion model.



Vocoder Generator: A separate generator produces batches of Mel spectrograms and audio waveforms, tailored for training the HiFi-GAN vocoder.



Voice Conversion Model





Architecture: The model employs a U-Net-like structure with Conv2D layers to transform male Mel spectrograms into female ones.





Encoder: Comprises three downsampling blocks, each with Conv2D layers, LeakyReLU activations, and MaxPooling2D for dimensionality reduction.



Bottleneck: Features two Conv2D layers with LeakyReLU activations to process the compressed representation.



Decoder: Includes three upsampling blocks with Conv2D layers and LeakyReLU activations, concluding with a Conv2D output layer.



Shape Adjustment: A Cropping2D layer adjusts the output to match the input dimensions precisely.



Training:





The model is optimized using the Adam optimizer with a learning rate of 1e-4 and Mean Absolute Error (MAE) as the loss function.



Training supports resuming from saved checkpoints and incorporates early stopping based on validation loss to prevent overfitting.



HiFi-GAN Vocoder





Generator:





Begins with a Conv1D layer, followed by upsampling blocks using Conv1DTranspose to increase resolution.



Each upsampling block is succeeded by residual blocks to enhance waveform quality.



Discriminator:





Utilizes a multi-scale architecture with Conv1D layers and LeakyReLU activations to evaluate waveform authenticity.



Loss Functions:





Discriminator Loss: Employs Least Squares GAN (LSGAN) loss to distinguish real from generated waveforms.



Generator Loss: Combines LSGAN adversarial loss with feature matching loss to improve synthesis quality.



Training:





Both generator and discriminator are trained with Adam optimizers, using specific beta values for stability.



The training alternates updates between the discriminator and generator to balance adversarial learning.



Inference Pipeline





Input Processing: The input audio is preprocessed into a Mel spectrogram following the same steps as in training.



Conversion: The trained U-Net model predicts a female Mel spectrogram from the male input spectrogram.



Synthesis:





The HiFi-GAN vocoder converts the predicted Mel spectrogram into an audio waveform.



If the vocoder is unavailable or fails, the script defaults to the Griffin-Lim algorithm for waveform reconstruction.



Error Handling and Logging





Robustness: The script includes extensive try-except blocks to manage potential errors during file loading, preprocessing, and model operations, ensuring uninterrupted execution.



Logging: Pythonâ€™s logging module is used to provide detailed feedback on each step, aiding in debugging and process monitoring.



Command-Line Interface





Flexibility: The script supports multiple operational modes (training, vocoder training, inference) via command-line arguments, enhancing its adaptability.



User Control: Options such as forcing data reprocessing, disabling the data generator, or specifying model checkpoints offer users granular control over the workflow.

This approach provides a comprehensive, step-by-step methodology for male-to-female voice conversion, leveraging robust data handling, advanced deep learning models, and efficient software engineering practices to achieve reliable results.